## Introduction:
The aviation industry is growing rapidly as airline companies look to retain and aquire new customers. Airlines can quickly adapt to the market in a competitive environment. Usually Price is used as the deciding competitive component. Passengers have also started considering an airline's services. In this analysis we will find out how the services affect a passenger's satisfaction.  <br>
<br>
To ensure profitability, aviation companies can offer quality services to improve customer satisfaction and create a loyal customer base. This will be rewarding for the airlines in the long run. <br>
<br>
Airline management can employ a customer driven approach to determine which services are desired the most by passengers. 

The U.S. Airline Passenger Satisfaction Dataset describes passenger satisfaction by conduct a survey at the airport after arriving in 2015. The dataset details each customer's information as it relates to age, gender, type of travel, flight distance, class type, departure and arrival delay.  

The dataset also contains 14 airline services of customer satisfaction levels ranging from values 0â€“5. The airline services for measured satis- faction level categories such: inflight wi-fi service, departure/arrival time convenience, ease of online booking, gate location, food and drink, online boarding, seat comfort, inflight entertainment, on-board service, leg-room service, baggage handling, check-in service, inflight service, and cleanliness.

#Importing the necessary libraries 


import time 
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
import seaborn as sns 
import eli5

import statsmodels.api as sm
from sklearn.decomposition import PCA
from sklearn import preprocessing
from sklearn.ensemble import RandomForestClassifier as rf
from eli5.sklearn import PermutationImportance
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.impute import KNNImputer
from sklearn import metrics 
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn import ensemble
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, plot_confusion_matrix, plot_roc_curve

import warnings
warnings.filterwarnings('ignore')

#Loading the training dataset 
train = pd.read_csv("/Users/mayankagarwal/Documents/Stat 432 Project/train.csv")

#Loading the testing dataset 
test = pd.read_csv("/Users/mayankagarwal/Documents/Stat 432 Project/test.csv")

#looking at the first 5 rows. 
train.head()

test.head()

Gender: Gender of the passengers (Female, Male)

Customer Type: The customer type (Loyal customer, disloyal customer)

Age: The actual age of the passengers

Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)

Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)

Flight distance: The flight distance of this journey

Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)

Departure/Arrival time convenient: Satisfaction level of Departure/Arrival time convenient

Ease of Online booking: Satisfaction level of online booking

Gate location: Satisfaction level of Gate location

Food and drink: Satisfaction level of Food and drink

Online boarding: Satisfaction level of online boarding

Seat comfort: Satisfaction level of Seat comfort

Inflight entertainment: Satisfaction level of inflight entertainment

On-board service: Satisfaction level of On-board service

Leg room service: Satisfaction level of Leg room service

Baggage handling: Satisfaction level of baggage handling

Check-in service: Satisfaction level of Check-in service

Inflight service: Satisfaction level of inflight service

Cleanliness: Satisfaction level of Cleanliness

Departure Delay in Minutes: Minutes delayed when departure

Arrival Delay in Minutes: Minutes delayed when Arrival

Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)

sns.countplot(x = 'Gender', data = train)

sns.countplot(x = 'Customer Type', data = train)

sns.countplot(x = 'Type of Travel', data = train)

sns.countplot(x = 'Class', data = train)

train.describe()

#Checking Info of our data -
train.info()

#Checking for Data Imbalance -
fig = plt.figure(figsize = (9,6))
train['satisfaction'].value_counts(normalize = True).plot(kind='bar', color= ['blue','yellow'], alpha = 0.9, rot=0)
plt.title('Satisfaction Level of customers')
plt.show()

train['satisfaction'].value_counts(normalize = True)

Hence we see that we have  56.67% of 'Neutral or Dissatisfied' customers as compared to 43.33% of 'Satisfied' customers and thus our data is almost balanced.

with sns.axes_style(style='ticks'):
    g = sns.catplot("satisfaction", col="Gender", col_wrap=2, data=train, kind="count", height=3, aspect=2.0)  
    g = sns.catplot("satisfaction", col="Customer Type", col_wrap=2, data=train, kind="count", height=3, aspect=2.0)

For both Males and Females, we see that no. of of 'Neutral or Dissatisfied' customers are more as compared to 'Satisfied' customers. The same logic applies to 'Loyal' and 'DisLoyal' Customers. Also we see that the no . of Loyal customers are way more than 'Disloyal' customers.

with sns.axes_style('dark'):
    g = sns.catplot("Age", data=train, aspect=4.0, kind='count', hue='satisfaction', order=range(6, 90))
    g.set_ylabels('Count of Satisfaction level at a certain age')

We see that our early age passengers i.e. 8-38 are more nuetral or dissatisfied as comapred to satisfied. Passengers aged 39-60 have higher satisfaction level and then the trend for dissatisfied or neutral is again higher for the remaining higher age group. 

with sns.axes_style('dark'):
    g = sns.catplot(x="Flight Distance", y="Type of Travel",
                    hue="satisfaction", col="Class", data=train, kind="boxen", height=5.5, aspect=.9)

 For business travel in business class category, th## For Eco Plus class, very inconvenient Departure/Arrival time (Departure/Arrival_time_convenient = 0) has really high no. of dissatisfied passengers, even when online boarding is done very well. For other combinations, no. of satisfied passengers are on the higher side compared to no. of dissatisfied passengers.e number of satisfied passengers are quite on the higher side for longer flight distance. For other combinations, there is almost equal distribution of satisfied and dissatisfied passengers.

For Eco Plus class, very inconvenient Departure/Arrival time (Departure/Arrival_time_convenient = 0) has really high no. of dissatisfied passengers, even when online boarding is done very well. For other combinations, no. of satisfied passengers are on the higher side compared to no. of dissatisfied passengers.


with sns.axes_style('dark'):
    g = sns.catplot(x="Departure/Arrival time convenient", 
                    y="Online boarding", hue="satisfaction", col="Class", 
                    data=train, kind="bar", height=5.5, aspect=.9)

with sns.axes_style('white'):
    g = sns.catplot(x="Class", y="Departure Delay in Minutes", hue="satisfaction", col="Type of Travel", data=train, kind="bar", height=4.5, aspect=.8)
    g = sns.catplot(x="Class", y="Arrival Delay in Minutes", hue="satisfaction", col="Type of Travel", data=train, kind="bar", height=4.5, aspect=.8)

For personal travel (specially Eco Plus and Eco), the no. of dissatisfied passengers are really high when arrival delay in minutes is high. As observed the level of dissatisfied or neutral passengers is always more than the satisfied passengers whenever there's a delay in arrival or departure time. 

For business class, it is observed that all gate locations have higher no. of dissatisfied passengers when baggage handling is not done perfectly well (rating <= 4). For Eco Plus, when the gate location is 1 and for Eco, when the gate location is 2, even when the baggages are handled in a mediocre way (rating in range 2.0 - 4.0), passengers remained dissatisfied.

with sns.axes_style('white'):
    g = sns.catplot(x="Gate location", y="Baggage handling", hue="satisfaction", col="Class", data=train, kind="boxen", height=4.5, aspect=.8)

The Eco Plus passengers are mostly satisfied without in-flight wi-fi service (rating 0) and medium level of in-flight entertainment (rating 2 - 4). For Business class passengers, only highest level of in-flight entertainment (rating 5) can make them satisfied. For Eco passengers, high level of in-flight entertainment (rating 3 - 5) and very high wi-fi service availability (rating 5) can make them satisfied.

g = sns.catplot(x="Inflight wifi service", y="Inflight entertainment", 
                hue="satisfaction", col="Class", data=train, kind="box", height=4.5, aspect=.8)

But naturally as seat comfort increases, the customer satisfaction increases as well.

with sns.axes_style(style='ticks'):
    g = sns.catplot("satisfaction", col="Seat comfort", 
                    col_wrap=6, data=train, kind="count", height=2.5, aspect=.8)

As cleanliness increases, the satisfaction level of passengers increases.

with sns.axes_style(style='ticks'):
    g = sns.catplot("satisfaction", col="Cleanliness", col_wrap=6, data=train, kind="count", height=2.5, aspect=.8)

The maximum no. of satisfied passengers belong to the category of 4 and 5 rating givers. 
Below rating 4, passengers are mostly dissatisfied.

with sns.axes_style(style='ticks'):
    g = sns.catplot("satisfaction", col="Food and drink", col_wrap=6, 
                    data=train, kind="count", height=2.5, aspect=.8)

corr = train.corr(method='spearman')
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(20, 18))
sns.heatmap(corr, annot = True, mask=mask, cmap="YlGnBu", center=0,
            square=True, linewidths=.5)

col_list = train.columns

col_list

## Cleaning and Transforming the Data 

### Checking for missing values in the dataset 

train.isna().sum()

test.isna().sum()

def impute(temp):
    imputer = KNNImputer(n_neighbors=2,metric='nan_euclidean')
    #Fit the dataset
    imputer.fit(temp[['Arrival Delay in Minutes']])

    # transform the dataset
    Y = imputer.transform(temp[['Arrival Delay in Minutes']]) 

    arrival_delay_imputer = pd.DataFrame(Y,columns=['Arrival Delay in Minutes']) 
    temp['Arrival Delay in Minutes']= arrival_delay_imputer['Arrival Delay in Minutes'].reset_index(drop=True) 

impute(train)
impute(test)

test.isna().sum()

#Dropping the first two columns
train = train.drop(['Unnamed: 0','id'], axis = 1)
test = test.drop(['Unnamed: 0','id'], axis = 1)

#Function to Transform the response variable 

def response(temp):
    if temp == "satisfied":
        return 1
    elif temp == 'neutral or dissatisfied':
        return 0

#Function to transform 'Gender' feature 

def gender(temp):
    if temp == "Male":
        return 0 
    elif temp == "Female":
        return 1 
    else:
        return -1 

#Function to transform 'Customer Type' feature 

def cust_type(temp):
    if temp == "Loyal Customer":
        return 1
    elif temp == "disloyal Customer":
        return 0
    else:
        return -1

#Function to transform 'Type of Travel' feature 

def type_travel(temp):
    if temp == "Business travel":
        return 1
    elif temp == "Personal Travel":
        return 0
    else:
        return -1 

#Function to transform 'Class' feature 

def class_type(temp):
    if temp == "Eco Plus":
        return 2
    elif temp == "Business":
        return 1
    elif temp == "Eco":
        return 0 
    else:
        return -1

#Function to perform the transformations on the dataset 

def transform_features(df_temp):
    df_temp['satisfaction'] = df_temp['satisfaction'].apply(response)
    df_temp['Gender'] = df_temp['Gender'].apply(gender)
    df_temp['Customer Type'] = df_temp['Customer Type'].apply(cust_type)
    df_temp['Type of Travel'] = df_temp['Type of Travel'].apply(type_travel)
    df_temp['Class'] = df_temp['Class'].apply(class_type)
    #df_temp = outlier(df_temp)
    
    return df_temp

train = transform_features(train)
test = transform_features(test)

train.head()

col_list = col_list[2:]

col_list

sns.boxplot('Flight Distance', data = train)
plt.rcParams["figure.figsize"] = (10,5)

sns.boxplot('Departure Delay in Minutes', data = train)
plt.rcParams["figure.figsize"] = (10,5)

sns.boxplot('Arrival Delay in Minutes', data = train)
plt.rcParams["figure.figsize"] = (10,5)

## Scaling the data so that all the predictors are given equal importance.
r_scaler = preprocessing.MinMaxScaler()
r_scaler.fit(train)
scaled_train = pd.DataFrame(r_scaler.transform(train), columns=train.columns)
scaled_train.head()

r_scaler.fit(test)
scaled_test = pd.DataFrame(r_scaler.transform(test), columns=test.columns)
scaled_test.head()

corr = scaled_train.corr(method='spearman')
# Generate a mask for the upper triangle
mask = np.zeros_like(corr, dtype=np.bool)
mask[np.triu_indices_from(mask)] = True
f, ax = plt.subplots(figsize=(20, 18))
sns.heatmap(corr, annot = True, mask=mask, cmap="YlGnBu", center=0,
            square=True, linewidths=.5)

From the above Heatmap we can observe that the factors affecting the passengers satisfaction the most are :
1. Online Boarding 
2. Type of travel 
3. Inflight entertainment 
4. Class 
5. Seat comfort 
6. On-board service 
7. Leg room service 
8. Cleanliness 
9. Inflight wifi service 
10. Baggage Handling 

The features which affect negatively on a passengers satisfaction are :
1. Arrival Delay in minutes  
2. Departure Delay in minutes 
3. Departure/Arrival time convenient 
4. Gender 
5. Gate location 

Also from the above correlation plot we see that some of the factors are correlated with one other (multicollinearity) 
1. Arrival delay in minutes and Departure delay in minutes (0.74)
2. Ease on online booking and Inflight wifi service (0.71)
3. Cleanliness with Inflight entertainment(0.68)
4. Cleanliness with Seat comfort (0.67)
5. Cleanliness with Food and drink (0.65) 
6. Inflight service and baggage handling (0.63) 
7. Inflight entertainment with Food and drink (0.61) 

## Chi Square Test for feature selection
X = scaled_train.loc[:,scaled_train.columns!='satisfaction']
y = scaled_train[['satisfaction']]
selector = SelectKBest(chi2, k=10)
selector.fit(X, y)
train_new = selector.transform(X)
print(X.columns[selector.get_support(indices=True)])

##Permutation test -
perm = PermutationImportance(rf(n_estimators=100, random_state=0).fit(X,y),random_state=1).fit(X,y)
eli5.show_weights(perm, feature_names = X.columns.tolist())

Results of Permutation Test: 


features = ['Type of Travel', 'Class', 'Flight Distance', 'Inflight wifi service',
       'Online boarding', 'Seat comfort', 'Inflight entertainment',
       'On-board service', 'Leg room service', 'Cleanliness']

test

X_train = scaled_train[features]
y_train = scaled_train['satisfaction']
X_test = scaled_test[features]
y_test = scaled_test['satisfaction']

X_train.head()

X_test.head()

len(X_train)

len(y_train)

## Classification Models 

### Logistic Regression 

Logistic regression builds a regression model to predict the probability that a particular data entry belongs to a particular class (1). It models the data using sigmoid function. 

##### Assumptions:
1. Target variable is binary and level 1 should define the desired outcome. 
2. Large sample size
3. Only important features must be considered. 
4. The model should have no or little multicollinearity. 

GridSearchCV tries all possibilities of hyperparameters. Hence the main drawback of this method is that it is computationally expensive. 

Our main aim is to minimize error to get the best predicted output. 
We try and minimize the cost function (quadratic function).

1. Newton's method : It uses a better quadratic function minimization. (using 1st and 2nd partial derivatives) using hessian matrix. At each iteration is approximates f(x) a quadratic functin and takes a step towards max/min of that function. <br>
Drawbacks:computationally expensive because of hessian matrix, It attracts to saddle points. 
<br>
2. Limited-memory Broyden-Fletcher-Goldfarb-Shanno (lbfgs):
It approximates hessian matrix using updates specified by gradient evaluations. (saves a few vectors that represent the approximation implicity). It is now the default in sklearn library for logistic. 
Drawbacks: It sometimes may not converge to anything 
<br>
3. A libray for Large Linear Classification: 
Uses coordinate Descent (CD) algorithm solving optimization problems by successively performing approximate minimization along coordinate directions or coordinate hyperplanes. It applies L1 regularilzation 
<br>
4. SAGA:
A variant of sag(Stochastic Average gradient descent). It supports L1 penalty and is therefore suitable for large datasets. 
sag optimizes the sum of a finite number of smooth convex functions. It is faster than other solvers for large datasets as it incorporates memory of previous gradient values. 


The most commonly used penalized regression include:

1. Ridge regression: variables with minor contribution have their coefficients close to zero. However, all the variables are incorporated in the model. This is useful when all variables need to be incorporated in the model according to domain knowledge.<br>
2. Lasso regression: the coefficients of some less contributive variables are forced to be exactly zero. Only the most significant variables are kept in the final model.<br>
3. Elastic net regression: the combination of ridge and lasso regression. It shrinks some coefficients toward zero (like ridge regression) and set some coefficients to exactly zero (like lasso regression)

logistic_model = LogisticRegression()
solvers = ['newton-cg','lbfgs', 'liblinear', 'saga']
penalty = ['l1','l2','elasticnet']
c_val = [100,10,1,0.1,0.01]
log_grid = dict(solver = solvers, penalty = penalty, C = c_val)
cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 123)
grid_search = GridSearchCV(estimator = logistic_model, param_grid = log_grid, n_jobs = -1,
                           cv = cv, scoring = 'accuracy', error_score = 0)
grid_results = grid_search.fit(X_train, y_train)

print("Best accuracy : %f using parameters %s"%(grid_results.best_score_,grid_results.best_params_))

means = grid_results.cv_results_['mean_test_score']
stds = grid_results.cv_results_['std_test_score']
params = grid_results.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

log_pred = grid_search.predict(X_test)

accuracy = accuracy_score(y_test, log_pred)
print("The accuracy of Logistic regression model is : {}".format(accuracy))

#AUC 
roc = roc_auc_score(y_test, log_pred)
print("ROC Area under the curve for Logistic model : {}".format(roc))
plot_roc_curve(grid_search, X_test, y_test)

print(classification_report(y_test, log_pred, digits=4))

plot_confusion_matrix(grid_search, X_test, y_test)

### Decision Trees 

decision = DecisionTreeClassifier()

decision_params = {
    'max_depth': [2, 3, 5, 10, 20],
    'min_samples_leaf': [5, 10, 20, 50, 100],
    'criterion': ["gini", "entropy"]
}

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 2, random_state=1)
grid_decision = GridSearchCV(estimator=decision, param_grid = decision_params, cv = cv,
                             n_jobs=-1,scoring = "accuracy")
decision_fit = grid_decision.fit(X_train, y_train)

print("Best accuracy : %f using parameters %s"%(decision_fit.best_score_,decision_fit.best_params_))

means = decision_fit.cv_results_['mean_test_score']
stds = decision_fit.cv_results_['std_test_score']
params = decision_fit.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

decision_pred = decision_fit.predict(X_test)

accuracy = accuracy_score(y_test, decision_pred)
print("The accuracy of decison tree model is : {}".format(accuracy))

#AUC 
roc = roc_auc_score(y_test, decision_pred)
print("ROC Area under the curve for decision tree model : {}".format(roc))
plot_roc_curve(decision_fit, X_test, y_test)

print(classification_report(y_test, decision_pred, digits=4))

plot_confusion_matrix(decision_fit, X_test, y_test)

#### Graphical visualization of the decision tree









### Random Forests 

def random_Forest_Optimization(X, y):
  # define models and parameters
  rf_model = RandomForestClassifier()
  estimator_values = [10, 100, 1000]
  max_feature_grid = ['sqrt', 'log2']

  # define grid search
  grid = dict(n_estimators=estimator_values,max_features=max_feature_grid)
  cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
  cv_grid = GridSearchCV(estimator=rf_model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
  rf_fit = cv_grid.fit(X, y)
  
  # summarize results
  print("Best: %f using %s" % (rf_fit.best_score_, rf_fit.best_params_))
  rf_mean = rf_fit.cv_results_['mean_test_score']
  rf_stds = rf_fit.cv_results_['std_test_score']
  rf_params = rf_fit.cv_results_['params']
  for mean, stdev, param in zip(rf_mean, rf_stds, rf_params):
      print("%f (%f) with: %r" % (mean, stdev, param))

random_Forest_Optimization(X_train, y_train)

rf = RandomForestClassifier(max_features = 'sqrt', n_estimators = 100)
rf.fit(X_train, y_train)

random_pred = rf.predict(X_test)
accuracy = accuracy_score(y_test, random_pred)
print("The accuracy of Logistic regression model is : {}".format(accuracy))

#AUC 
roc = roc_auc_score(y_test, random_pred)
print("ROC Area under the curve for Logistic model : {}".format(roc))
plot_roc_curve(rf, X_test, y_test)

print(classification_report(y_test, random_pred, digits=4))

plot_confusion_matrix(rf, X_test, y_test)

### K-Nearest Neighbours 


def knn_Optimization(X,y):
    knn_model = KNeighborsClassifier()
    neighbors_grid = range(1,10, 1)
    weight_grid = ['uniform', 'distance']
    metric_grid = ['euclidean', 'manhattan', 'minkowski']

    # define grid search
    knn_grid = dict(n_neighbors=neighbors_grid,weights=weight_grid,metric=metric_grid)
    knn_cv = RepeatedStratifiedKFold(n_splits=10, n_repeats = 2 , random_state=1)
    knn_fit = GridSearchCV(estimator=knn_model, param_grid=knn_grid, n_jobs=-1, cv=knn_cv, scoring='accuracy',error_score=0)
    knn_res = knn_fit.fit(X, y)

    # summarize results
    print("Best: %f using %s" % (knn_res.best_score_, knn_res.best_params_))
    mean_value = knn_res.cv_results_['mean_test_score']
    std_value = knn_res.cv_results_['std_test_score']
    parameters = knn_res.cv_results_['params']
    for mean, stdev, param in zip(mean_value, std_value, parameters):
      print("%f (%f) with: %r" % (mean, stdev, param))
    
    return knn_fit

knnmod = knn_Optimization(X_train,y_train)

knn_pred = knnmod.predict(X_test)

accuracy = accuracy_score(y_test, knn_pred)
print("The accuracy of Logistic regression model is : {}".format(accuracy))

#AUC 
roc = roc_auc_score(y_test, knn_pred)
print("ROC Area under the curve for K-Nearest neighbours : {}".format(roc))
plot_roc_curve(knnmod, X_test, y_test)

print(classification_report(y_test, knn_pred, digits=4))

plot_confusion_matrix(knnmod, X_test, y_test)

## SVM 
svm_model = SVC()

kernel = ['poly', 'rbf', 'sigmoid']
C = [50, 10, 1.0, 0.1, 0.01]
gamma = ['scale']

grid_svm = dict(kernel=kernel,C=C,gamma=gamma)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
svm_search = GridSearchCV(estimator=svm_model, param_grid=grid_svm, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
grid_result = svm_search.fit(X, y)

# summarize results
print("Best: %f using %s" % (grid_result.best_score_, grid_result.best_params_))
means = grid_result.cv_results_['mean_test_score']
stds = grid_result.cv_results_['std_test_score']
params = grid_result.cv_results_['params']
for mean, stdev, param in zip(means, stds, params):
    print("%f (%f) with: %r" % (mean, stdev, param))

svm_pred = svm_search.predict(X_test)

accuracy = accuracy_score(y_test, svm_pred)
print("The accuracy of Logistic regression model is : {}".format(accuracy))

#AUC 
roc = roc_auc_score(y_test, svm_pred)
print("ROC Area under the curve for K-Nearest neighbours : {}".format(roc))
plot_roc_curve(svm_search, X_test, y_test)

print(classification_report(y_test, svm_pred, digits=4))

plot_confusion_matrix(svm_search, X_test, y_test)

### Ada Boost 





